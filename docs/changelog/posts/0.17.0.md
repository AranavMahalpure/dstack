---
date: 2024-03-28
description: "The latest update previews service replicas and auto-scaling, and brings many other improvements."
slug: "0.17.0"
template: blog-post-0.17.0-workaround.html
---

# dstack 0.17.0: Service auto-scaling, and other improvements

The latest update previews service replicas and auto-scaling, and brings many other improvements.

<!-- more -->

## Service auto-scaling

Previously, `dstack` always served services as single replicas. While this is suitable for development, in production, the
service must automatically scale based on the load.

That's why in `0.17.0`, we extended `dstack` with the capability to configure the number of 
replicas as well as the auto-scaling policy.

<div editor-title="serve.dstack.yml">

```yaml
type: service

python: "3.11"
env:
  - MODEL=NousResearch/Llama-2-7b-chat-hf
commands:
  - pip install vllm
  - python -m vllm.entrypoints.openai.api_server --model $MODEL --port 8000
port: 8000

replicas: 1..4
scaling:
  metric: rps
  target: 10

# (Optional) Enable the OpenAI-compatible endpoint
model:
  format: openai
  type: chat
  name: NousResearch/Llama-2-7b-chat-hf
```

</div>

The `replicas` property can be set either to a number or to a range. In the case of a range, the `scaling` property is
required to configure the auto-scaling policy. 
The auto-scaling policy requires specifying `metric` (such as `rps`, i.e. "requests per second") and its `target` 
(the metric value).

## Regions and instance types

Also, the update brings a simpler way to configure regions and instance types.

For example, if you'd like to use only a subset of specific regions or instance types,
you can now configure them via `.dstack/profiles.yml`.

<div editor-title=".dstack/profiles.yml">

```yaml
profiles:
  - name: custom
    default: false

    regions:
      - us-east-1
      - us-east-2

    instance_types:
      - p3.2xlarge
      - p3.8xlarge
      - p3.16xlarge
```

</div>


Then you can pass it to `dstack run` with `--profile custom`. Alternatively, you can set `default` to `true`, and then 
`dstack run` will apply it automatically.

If you don't want to define a profile, you can use the `--region` and `--instance-type` options directly
with `dstack run`.

## Environment variables

Previously, environment variables had to be hardcoded in the configuration file or passed via the CLI. The update brings
two major improvements.

Firstly, it's now possible to configure an environment variable in the configuration without hardcoding its value. 
Secondly, `dstack run` now inherits environment variables from the current process.

Together, these features allow users to define environment variables separately from the configuration and pass them to
`dstack run` conveniently, such as by using a `.env` file.

<div editor-title="train.dstack.yml">

```yaml
type: task

python: "3.11"

env:
  - HUGGING_FACE_HUB_TOKEN
  - HF_HUB_ENABLE_HF_TRANSFER=1

commands:
  - pip install -r fine-tuning/qlora/requirements.txt
  - tensorboard --logdir results/runs &
  - python fine-tuning/qlora/train.py --merge_and_push ${{ run.args }}
ports:
  - 6006

resources:
  gpu: 16GB..24GB
```

</div>

Now, if you run this configuration, `dstack` will ensure that you've set `HUGGING_FACE_HUB_TOKEN`
(either via `HUGGING_FACE_HUB_TOKEN=<value> dstack run ...`, `dstack run -e HUGGING_FACE_HUB_TOKEN=<value> ...`,
or by using other tools such as `direnv` or similar.

## Feedback

Have questions or need help? Drop us a message on our [Discord server](https://discord.gg/u8SmfwPpMd)!