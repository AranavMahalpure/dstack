per_device_train_batch_size: 24
per_device_eval_batch_size: 8
num_train_epochs: 1
max_steps: -1
output_dir: "./finetuned_models/llama3_fine_tuned"
optim: "adafactor"
dataset_name: "databricks/databricks-dolly-15k"
model_name: "meta-llama/Meta-Llama-3.1-8B"
lora_r: 8
max_seq_length: 1024
packing: True
push_to_hub: True