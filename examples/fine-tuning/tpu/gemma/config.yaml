per_device_train_batch_size: 24
per_device_eval_batch_size: 8
num_train_epochs: 1
max_steps: -1
output_dir: "./finetuned_models/gemma-2b-fine_tuned"
optim: "adafactor"
dataset_name: "databricks/databricks-dolly-15k"
model_name: "google/gemma-2b"
lora_r: 8
max_seq_length: 1024
packing: True
push_to_hub: True