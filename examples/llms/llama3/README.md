# Llama 3 70B

The following command deploys Llama 3 70B as a service:

```shell
dstack run . -f examples/llms/llama3/ollama.dstack.yml
```

See the configuration at [ollama.dstack.yml](ollama.dstack.yml).

For more details, refer to [services](https://dstack.ai/docs/concepts/services).