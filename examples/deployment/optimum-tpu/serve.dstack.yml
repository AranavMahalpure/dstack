type: service
name: llama31-service-optimum-tpu
# This service runs meta-llama/Meta-Llama-3.1-8B with TGI
# Refer to Note section in examples/gpus/tpu/README.md for more information about the image.
image: sjbbihan/optimum-tpu:latest

# Required environment variables
env:
  - HUGGING_FACE_HUB_TOKEN
  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B
commands:
  - text-generation-launcher --port 8000 --max-concurrent-requests 4 --max-input-tokens 128 --max-total-tokens 150 --max-batch-prefill-tokens 128
port: 8000

# Use either spot or on-demand instances
spot_policy: auto

resources:
  gpu: v5litepod-8

# (Optional) Enable the OpenAI-compatible endpoint
model:
  format: tgi
  type: chat
  name: meta-llama/Meta-Llama-3.1-8B