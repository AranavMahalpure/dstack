type: service
# The name is optional, if not specified, generated randomly
name: llama31-service-optimum-tpu

# Using a Docker image with a fix instead of the official one
# More details at https://github.com/huggingface/optimum-tpu/pull/85
image: sjbbihan/optimum-tpu:latest
# Required environment variables
env:
  - HUGGING_FACE_HUB_TOKEN
  - MODEL_ID=meta-llama/Meta-Llama-3.1-8B
  - MAX_CONCURRENT_REQUESTS=4
  - MAX_INPUT_TOKENS=128
  - MAX_TOTAL_TOKENS=150
  - MAX_BATCH_PREFILL_TOKENS=128
commands:
  - text-generation-launcher --port 8000
port: 8000

resources:
  # Required resources
  gpu: v5litepod-8

# Use either spot or on-demand instances
spot_policy: auto

model:
  format: tgi
  type: chat
  name: meta-llama/Meta-Llama-3.1-8B